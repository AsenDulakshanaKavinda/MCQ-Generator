{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdd7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from fastapi import UploadFile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(paths: Iterable[Path]) -> List[Document]:\n",
    "    print(\"documents loading ...\")\n",
    "    print(f\"documents about to load \\n {str(paths)}\")\n",
    "\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    try:\n",
    "        for path in paths:\n",
    "            ext = path.suffix.lower()\n",
    "            print(f\"trying to load {path} with {ext}\")\n",
    "            if ext == \".pdf\":\n",
    "                loader = PyPDFLoader(str(path))\n",
    "            elif ext == \".docx\":\n",
    "                loader = Docx2txtLoader(str(path))\n",
    "            elif ext == \".txt\":\n",
    "                loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "            else:\n",
    "                print(f\"Unsupported extension skipped, path={str(path)}\")\n",
    "                continue\n",
    "            docs.extend(loader.load())\n",
    "            print(docs)\n",
    "        print(f\"{len(docs)} Documents loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading documents, error={str(e)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents loading ...\n",
      "documents about to load \n",
      " [WindowsPath('E:/Project/MCQ-Generator/test_data/test.txt')]\n",
      "trying to load E:\\Project\\MCQ-Generator\\test_data\\test.txt with .txt\n",
      "[Document(metadata={'source': 'E:\\\\Project\\\\MCQ-Generator\\\\test_data\\\\test.txt'}, page_content='some test data here\\n')]\n",
      "1 Documents loaded\n"
     ]
    }
   ],
   "source": [
    "test_files = [Path(\"E:/Project/MCQ-Generator/test_data/test.txt\")]\n",
    "load_documents(test_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\", \".pptx\", \".md\", \".csv\", \".xlsx\", \".xls\", \".db\", \".sqlite\", \".sqlite3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_uploaded_files(uploaded_files: Iterable, target_dir: Path) -> List[Path]:\n",
    "\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    saved: list[Path] = []\n",
    "\n",
    "    for uf in uploaded_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7404b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 unique multiple-choice questions (MCQs) based on the provided context about **Natural Language Processing (NLP)**:\n",
      "\n",
      "---\n",
      "\n",
      "### **Question 1**\n",
      "What was the **highest F1 score** achieved in the **CoNLL 2003 Named Entity Recognition (NER) challenge** as mentioned in the text?\n",
      "\n",
      "**A)** 86.84%\n",
      "**B)** 88.31%\n",
      "**C)** 88.76%\n",
      "**D)** 89.31%\n",
      "\n",
      "✅ **Correct Answer: D) 89.31%**\n",
      "\n",
      "---\n",
      "\n",
      "### **Question 2**\n",
      "According to the text, what is a **common traditional approach** in NLP for tasks like POS tagging, chunking, and NER?\n",
      "\n",
      "**A)** Using deep neural networks without feature engineering\n",
      "**B)** Extracting hand-designed features and feeding them into shallow classifiers (e.g., SVM)\n",
      "**C)** Relying solely on unlabeled data without any feature selection\n",
      "**D)** Applying rule-based systems without machine learning\n",
      "\n",
      "✅ **Correct Answer: B) Extracting hand-designed features and feeding them into shallow classifiers (e.g., SVM)**\n",
      "\n",
      "---\n",
      "\n",
      "### **Question 3**\n",
      "Which of the following was **NOT** mentioned as a feature used by the **best-performing systems in the CoNLL 2003 NER challenge**?\n",
      "\n",
      "**A)** Words and POS tags\n",
      "**B)** Prefixes and suffixes\n",
      "**C)** A large gazetteer (external list of named entities)\n",
      "**D)** Pre-trained transformer models (e.g., BERT)\n",
      "\n",
      "✅ **Correct Answer: D) Pre-trained transformer models (e.g., BERT)**\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- 1. Setup Vector Store and Retriever (Prerequisite RAG steps) ---\n",
    "# In a real application, you would load and chunk your data first\n",
    "# For example, loading a PDF, splitting, embedding, and storing in Chroma\n",
    "loader = PyPDFLoader(\"E:/Project/MCQ-Generator/data/test_data/nlp.pdf\") # Replace with your document\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=MistralAIEmbeddings(model=\"mistral-embed\"))\n",
    "\n",
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Assuming you already have your vectorstore set up as `vectorstore` and `retriever`\n",
    "# from the previous code snippet:\n",
    "\n",
    "# ... (Previous code to set up vectorstore and retriever remains the same) ...\n",
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever(k=4) # Retrieve 4 relevant chunks\n",
    "\n",
    "# --- 1. Define the MCQ Prompt Template ---\n",
    "# The prompt is updated to generate questions based on the topic and context\n",
    "mcq_system_prompt = (\n",
    "    \"You are a helpful assistant that generates multiple-choice questions (MCQs) \"\n",
    "    \"based on provided context and a given topic.\\n\"\n",
    "    \"Generate exactly 3 unique MCQs about the topic '{topic}'. \"\n",
    "    \"For each MCQ, provide 4 options (A, B, C, D) and specify the correct answer.\\n\"\n",
    "    \"Use only the following context:\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Format your output clearly, with each question numbered.\"\n",
    ")\n",
    "\n",
    "mcq_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", mcq_system_prompt),\n",
    "        (\"human\", \"Generate the MCQs now based on the topic: '{topic}'\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 2. Initialize the LLM ---\n",
    "# Use a slightly higher temperature for creativity in generating options\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0.5)\n",
    "\n",
    "# --- 3. Build the MCQ Generation Chain using LCEL ---\n",
    "\n",
    "# We use RunnableParallel to manage the flow of inputs.\n",
    "# The user provides a 'topic'.\n",
    "# We need to use that 'topic' to retrieve 'context' AND pass the 'topic' to the final prompt.\n",
    "mcq_chain = (\n",
    "    RunnableParallel(\n",
    "        # 'context' key gets populated by feeding the 'topic' to the retriever\n",
    "        context=lambda x: retriever.invoke(x['topic']), \n",
    "        # 'topic' key simply passes the original input topic through\n",
    "        topic=RunnablePassthrough() \n",
    "    )\n",
    "    | mcq_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- 4. Invoke the chain with a user-specified topic ---\n",
    "user_topic = \"nlp\"\n",
    "response = mcq_chain.invoke({\"topic\": user_topic})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
