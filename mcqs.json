[
    {
        "question": "What was the primary reason the authors chose to rely on multilayer neural networks (a 20-year-old technology at the time) for their NLP tasks?",
        "options": {
            "A": "They were the only available technology for NLP at the time.",
            "B": "Their ability to discover hidden representations using a stochastic learning algorithm that scales linearly with the number of examples.",
            "C": "They required less computational power compared to modern alternatives.",
            "D": "They were explicitly optimized for semantic role labeling (SRL) tasks."
        },
        "correct_answer": "B",
        "explanation": "The authors explicitly state in the context that they were attracted to multilayer neural networks because of their ability to discover hidden representations using a stochastic learning algorithm that scales linearly with the number of examples. This scalability allowed them to benefit from advances in computer hardware."
    },
    {
        "question": "In the context of Semantic Role Labeling (SRL), what do the tags ARG0-5 typically represent?",
        "options": {
            "A": "Syntactic roles such as subject, object, or indirect object in a sentence.",
            "B": "Modifier roles such as locational (ARGM-LOC) or temporal (ARGM-TMP).",
            "C": "Core arguments of a predicate (verb), where the precise arguments depend on the verb’s frame.",
            "D": "Parts of speech (POS) tags like noun, verb, or adjective."
        },
        "correct_answer": "C",
        "explanation": "The context describes ARG0-5 as tags representing the core arguments of a predicate (verb), with the precise arguments depending on the verb’s frame. Modifier roles like ARGM-LOC or ARGM-TMP are separate tags."
    },
    {
        "question": "Which dataset was used as the benchmark for Semantic Role Labeling (SRL) in the provided context?",
        "options": {
            "A": "CoNLL 2000 shared task.",
            "B": "CoNLL 2005 shared task.",
            "C": "WSJ sections 15–18 for training and section 20 for testing.",
            "D": "Brown corpus sections 1–10."
        },
        "correct_answer": "B",
        "explanation": "The context explicitly mentions that the CoNLL 2005 shared task was chosen as the SRL benchmark. It details the specific sections of the WSJ and Brown corpus used for training, validation, and testing."
    },
    {
        "question": "What is the primary criticism addressed in the context regarding the authors' use of neural networks?",
        "options": {
            "A": "Neural networks were too computationally expensive for NLP tasks.",
            "B": "The technology was outdated and lacked modern advancements.",
            "C": "Neural networks required excessive task-specific engineering.",
            "D": "The authors did not provide enough empirical evidence for their claims."
        },
        "correct_answer": "B",
        "explanation": "The context directly addresses the criticism of relying on a 'twenty-year-old technology' (multilayer neural networks) by explaining their scalability advantages and how hardware progress made them viable."
    },
    {
        "question": "What is the purpose of 'chunking' (or shallow parsing) in NLP, as described in the context?",
        "options": {
            "A": "To assign semantic roles (e.g., ARG0, ARGM-LOC) to words in a sentence.",
            "B": "To label segments of a sentence with syntactic constituents like noun phrases (NP) or verb phrases (VP).",
            "C": "To disambiguate word senses in a given text.",
            "D": "To generate parse trees for dependency parsing."
        },
        "correct_answer": "B",
        "explanation": "The context defines chunking (or shallow parsing) as the task of labeling segments of a sentence with syntactic constituents such as noun phrases (NP) or verb phrases (VP)."
    },
    {
        "question": "Which of the following is NOT mentioned as a feature used in state-of-the-art SRL systems in the context?",
        "options": {
            "A": "Part-of-speech (POS) tags.",
            "B": "Head words in a parse tree.",
            "C": "Word embeddings from pre-trained language models like BERT.",
            "D": "Path in the parse tree from the predicate to the argument."
        },
        "correct_answer": "C",
        "explanation": "The context lists features like POS tags, head words, phrase type, and parse tree paths but does not mention word embeddings from modern pre-trained language models like BERT."
    },
    {
        "question": "What was the F1-score achieved by Kudoh and Matsumoto (2000) in the CoNLL 2000 shared task on chunking?",
        "options": {
            "A": "97.16%",
            "B": "93.48%",
            "C": "97.33%",
            "D": "89.52%"
        },
        "correct_answer": "B",
        "explanation": "The context states that Kudoh and Matsumoto won the CoNLL 2000 challenge on chunking with an F1-score of 93.48%. The other scores (97.16% and 97.33%) are associated with POS tagging, not chunking."
    },
    {
        "question": "What is the role of the 'lookup table' (LTW) in the sentence approach network described in Figure 2 of the context?",
        "options": {
            "A": "To store pre-computed parse trees for faster processing.",
            "B": "To map discrete word features (e.g., stems, capitalization) to continuous vector representations.",
            "C": "To perform Viterbi decoding for sequence labeling tasks.",
            "D": "To generate semantic role labels (e.g., ARG0) directly from raw text."
        },
        "correct_answer": "B",
        "explanation": "The context describes the lookup table (LTW) as a mechanism to associate each discrete feature of a word (e.g., its stem or capitalization) with a continuous vector representation (parameters Wk). This is a key step in embedding words for neural network processing."
    },
    {
        "question": "Which of the following is described as a 'reduced objective' in NLP due to unresolved fundamental problems (e.g., lack of consensus on data structures for meaning representation)?",
        "options": {
            "A": "Developing end-to-end neural models for full textual understanding.",
            "B": "Extracting simpler representations like part-of-speech tags or semantic roles.",
            "C": "Training models on the entire Brown corpus without validation splits.",
            "D": "Using gazetteers to replace all named entity recognition (NER) systems."
        },
        "correct_answer": "B",
        "explanation": "The context explains that, due to unresolved fundamental problems (e.g., no consensus on how to represent textual meaning), researchers must settle for 'reduced objectives' like extracting simpler representations such as POS tags or semantic roles."
    },
    {
        "question": "What is the significance of training algorithms that scale linearly with the number of examples, as highlighted in the context?",
        "options": {
            "A": "They reduce the need for task-specific feature engineering.",
            "B": "They can leverage advances in computer hardware more effectively, as training time remains proportional to data size.",
            "C": "They eliminate the need for validation sets in NLP tasks.",
            "D": "They are only applicable to shallow parsing (chunking) and not to SRL."
        },
        "correct_answer": "B",
        "explanation": "The context emphasizes that linearly scaling algorithms benefit tremendously from progress in computer hardware, as training time grows proportionally with the number of examples. This makes them practical for large datasets as hardware improves."
    }
]